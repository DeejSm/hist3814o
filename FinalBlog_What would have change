I have lost all sense of where I am in the course. I simply have lost too much time and fallen too far behind. What I can do is try to sum up what I have learned. I also have been thinking about what would have changed when I worked on my MA and PhD, had I beeen immersed in this form of history when I started. Perhaps we could say what the grad student of ten years from now might be doing as a matter of course.
I have been introduced to new places in which to work -- for example Git Hub where I am now. So I have gone beyond dropbox. I am not convinced though that I will use these. I am either old-fashioned or paranoid. Hard to say which but while I do not trust the big corporations, I also do not trust the cloud. My data does not sit in some kind of heavenly ether. It is on a physical computer somewhere. If I trust blindly in, as one example, google drive, there is a likelihood my "stuff" sits on a computer somewhere in the United States and is thereby subject to American laws. Years ago I worked as an Industry Canada representative on a project dealing with lawful intercept of telecom and internet communications. I came out of that project even more distrustful than I had been going in. I want my "stuff" to be under my physical control.
I was forced in the course to articulate my reaction against large claims that digital history is wonderfully revolutionary. I found as I rlearned more about those writing on the subject that they were not naively wearing rose-glasses. Perhaps the large claims have been necessary to bring attention to the potential and the credibiity of digital history. My own sense as I worked through the assigned readings is that digital history is an extension not a fundamental change. The  requirement that evidence be the central building block of arguments, that we question the evidence and that we ask the so-what questions are as important as ever. There is one thing that has changed and that is the range of forms in which sources come to us.  Again, however, it is an extension. Just as in the past historians had to shift from reading the physical document itself, with all the attendent dust and grime, to squinting at microfiches and microforms, so historians now are stretched further to consider how to understand digitized records. We still have the same questions - why was the source created in the first place, why was it preserved across the years and, now, why you have access to it in whatever format, whether original, microform/microfiche or digital. And for all of these points the same questions have to be answered -- by whom, for whom, in what context, for what purpose, what has been lost versus what has been preserved.
Working in markdown, dillinger and GitHub has brought me face to face with my unthinking acceptance of Word. I realize I have enjoyed too much the sense of accomplishment by seeing my work immediately "look good" and have not considered the price of this polish. Certainly I grumble about the defaults which force me into Word's idea of the proper font, spacing between paragraphs and so. As a TA I have exhorted students to turn off defaults and been amazed each year at the ignorance of many in this computer-savy generation with the workings of the software they use. As I lay-out for print an Association's newsletters, I curse it for its unfitness for the task. And I have suffered near melt-downs when time after time it threw my dissertation into an endless pagination due to a particular graphic in chapter five. Yet there are some advantages in GUI-created documents. Automatic save is a wonderful device and works far more reliably than me remembering to stop and save. Undo is another marvelous function which I often wish worked in real life but which I have discovered does not work in GitHub when I accidently erase an entire line. I may continue to use a Text Wrangler which I installed at the suggestion of Shawn Anctil (although it appears it is already "retired"). I am certain I would not have used GitHub to exchange drafts and comments with my dissertation supervisor. As Sean Graham has said elsewhere in his final comments -- know your audience.
Where I really found grist for my mill was in modules two and three -- the finding and cleaning up of data. I was simply amazed to see an entire section of the war diaries appear on my computer while I used the five minutes it took to make a quick lunch. It is also scary. I have authored a virtual exhibit - mobilityhistories on omeka. Had I mirrored it using wget I would have saved myself time this winter when I had to rebuild the exhibit. I had kept a Word version of the text with notes as to the file names of the graphics so it was a case of working back and forth. Using a mirrored version would have been faster. At the same time, anyone can do to me what I did to LAC. I have pride of authorship so would not like to see my work stolen or misrepresented. As well, though, II have graphics in this exhibit which are not mine. I was given permission to use them by various entities. How much responsibility do I have to protect this material from misuse?
Text Encoding was neat and fun, although I found the specific commands difficult to read both on the screen and in the printed version. The exercise called for us to simply copy/paste and then replace with the appropriate material for the page being dissected. Copy/paste would not do for carrying this kind of work forward as I would want finer breakdowns of categories. For example I built an index of major items of interest as I read my newspaper which included differentiating between events on which it was reporting and events it initiated. There was also a clear and visible difference between its editorial opinion writing and its muckraking advocacy. These additional categories probably can be encoded but I would have to dissect what this command is made up of and how it works. The module does provide a link to a "short and sweet handout" which I still need to look at. I also had to be aware of time when I was doing my close reading of the newspaper. Transcribing relevant material from ten years of newspaper articles would have been daunting. 
Cleaning up messy data was something I did by eye, scanning for my sins every page of nearly forty years of the rural newspaper. I built an index as I read which which allowed me to pull together articles across time for examanation. I also transcribed scores of articles which were particulary relevant. I suspect it took not *too much* more time than encoding the material I had transcribed or even all that much more time than using Tesserat to turn my "pictures" of the newspaper into text. (Although Shawn Anctil did urge on me exploring software that would would "automate the boring stuff"). Which takes me to python (as recommended by Shawn), Tesseract, API, Regular Expressions. I did not achieve much of API as I have run out of time. What little I did left me with the sense that there is potential here. I did not get to do anything with Tesseract or the exercises to module 3 except read the directions. I intend to test these with a page from my newspaper but have not yet done so. It is possible this may have saved me a great deal of time. On the other hand, the transcriptions I did do on key articles left me reallly knowing them in a way an automated process would not have.
Visualizing data is something I had to do for both the MA and the PhD. I tried different approaches based on the capabilities which came packaged in Excel and chose what I thought made sense. I found the discussion in the module useful for intellecualizing the kind of decision-making I had to do but it would not have changed what I did. The drawbacks really lie in the software used as became clear when I discovered Excel had removed the capability of black-and-white graphing. Pretty colours which do not repoduce well had become the name of the game. What I *would* have done differently had I had the time for the MA was learn some good map-making software. The hours I spent copying via transparency drafting paper a soil map of Prescott County and then adding the villages and towns with the main highways and finally a map with circles to show economic growth of the towns! Marrying geographic and non-geographic information is something I expect to continue to do and it is something I would profit from learning.
The course has been frustrating. I lost so much time in the first two weeks due to technical errors, not "seeing" highlighted material intended to bring me to work tools (for example the textwrangler template) and I ended by not in fact exploring material that is potentially useful. Again going back to my days with Shawn Anctil, he had pointed me to the Programming Historian. I started to follow it the summer after the comps when I had more time and the dissertation seemed far away. It is time to go back to it and teach myself more.
