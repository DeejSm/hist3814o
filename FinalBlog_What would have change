#What I have Achieved and What I have Learned#
I simply have lost too much time and fallen too far behind. What I can do is try to sum up what I have learned. I also have been thinking about what would have changed when I worked on my MA and PhD, had I beeen immersed in digital history when I started. Perhaps we could say what the grad student of ten years from now might be doing as a matter of course.
I have been introduced to new places in which to work -- for example Git Hub where I am now. So I have gone beyond dropbox. I am not convinced though that I will use these. I am either old-fashioned or paranoid. Hard to say which but while I do not trust the big corporations, I also do not trust the cloud. My data still sits on a physical computer somewhere. Years ago I worked as an Industry Canada representative on a project dealing with lawful intercept of telecom and internet communications. I came out of that project even more distrustful than I had been going in. I want my "stuff" to be under my physical control.
I was forced in the course to articulate my reaction against large claims that digital history is wonderfully revolutionary. I found as I read more deeply that those writing on the subject were not naively wearing rose-glasses. Perhaps the large claims have been necessary to bring attention to the potential and the credibiity of digital history. My own sense as I worked through the assigned readings is that digital history is an extension not a fundamental change. The  requirement that evidence be the central building block of arguments, that we question the evidence and that we ask the so-what questions are as important as ever. There is one thing that has changed and that is the range of forms in which sources come to us.  Again, however, it is an extension. Just as in the past historians had to shift from reading the physical document itself, with all the attendent dust and grime, to squinting at microfiches and microforms, so historians now are stretched further to understand digitized records. We still have the same questions - why was the source created in the first place, why was it preserved across the years and, now, why do we have access to it in whatever format, whether original, microform/microfiche or digital. And for all of these points the same questions have to be answered -- by whom, for whom, in what context, for what purpose, what has been lost versus what has been preserved.
Working in markdown, dillinger and GitHub has brought me face to face with my unthinking acceptance of Word. I realize I have enjoyed too much the sense of accomplishment by seeing my work immediately "look good" and have not considered the price of this polish. Certainly I grumble about the defaults which force me into Word's idea of the proper font, spacing between paragraphs and so. As a TA I exhorted students to turn off defaults and was amazed each year at the ignorance of many in this computer-savy generation with the workings of the software they use. As I did lay-out of an Association's newsletters for which I was editor, I cursed it for its unfitness for the task. And I have suffered near melt-downs when time after time it threw my dissertation into an endless pagination due to a particular graphic in chapter five. Yet there are some advantages in GUI-created documents. Automatic save is a wonderful device and works far more reliably than me remembering to stop and save. Undo is another marvelous function which I often wish worked in real life but which I have discovered does not work in GitHub when I accidently erased an entire line. I may continue to use Text Wrangler which I installed earlier at the suggestion of Shawn Anctil (although it appears it is already "retired"). I am certain I would not have used GitHub to exchange drafts and comments with my dissertation supervisor. As Sean Graham has said elsewhere in his final comments -- know your audience.
Where I really found grist for my mill was in modules two and three -- the finding and cleaning up of data. I was simply amazed to see an entire section of the war diaries appear on my computer while I used the five minutes it took to make a quick lunch. It is also scary. I have authored a virtual exhibit - mobilityhistories on omeka. Had I mirrored it using wget I would have saved myself time this winter when I had to rebuild the exhibit. I had kept a Word version of the text with notes as to the file names of the graphics so it was a case of working back and forth. Using a mirrored version would have been faster. At the same time, anyone can do to me what I did to LAC. I have pride of authorship so would not like to see my work stolen or misrepresented. As well, though, I have graphics in this exhibit which are not mine. I was given permission to use them by various entities. How much responsibility do I have to protect this material from misuse?
Text Encoding was neat and fun, although I found the specific commands difficult to read both on the screen and in the printed version (a running problem for me has been distinguishing between i, l, and the numeral 1 due to poor eyesight made worse by dyslexia). The exercise called for us to simply copy/paste and then replace with the appropriate material for the page being dissected. Copy/paste would not do for carrying this kind of work forward as I would want finer breakdowns of categories. For example I built an index of major items of interest as I read my newspaper which included differentiating between events on which it was reporting and events it initiated. There was also a clear and visible difference between its editorial opinion writing and its muckraking advocacy. These additional categories probably can be encoded but I would have to dissect what this command is made up of and how it works. The module does provide a link to a "short and sweet handout" which I still need to look at. I also had to be aware of time when I was doing my close reading of the newspaper. Transcribing relevant material from ten years of newspaper articles would have been daunting.

#From this past week's fail log - on attempting to ocr the Ottawa Valley Journal#
Duplicating two issues of the newspaper from my computer, one jpg and one pdf, deletin all but two pages with rural content and makin a copy of the pdf as jpeg. I then dragged the the files onto dhbox home/deejsmith/
For the Tesseract exercise following lines 1 to 4 to import tesseract and image magick. [had to redo the image magic import as it was not working. had forgotten to use the sudo command.]
Use image magick on the newspaper files with conversion command [went nowhere - first attempt had misspelled and second had no clue what was wrong]. 
Doing the command as laid out in the workbook with the war diary file. [This time no hiccups but no action. The instructions said be patient. I was patient but the computer timed out instead.]
Beginning again on Saturday.
Reimport Tesseract and ImageMagick. Reinput the conversion command correcting a possible misreading. It worked and did about as reasonable a job as I expected on the war diary - would not take much to clean this up. Repeated the exercise substituting my file names to convert the newspaper. Worked but it is unreadable.

#More Thoughts on The Exercises#
Cleaning up messy data was something I did for the dissertation by eye, reading for my sins every page of nearly forty years of the rural newspaper which had been put on low quality microfilm. I built an index as I read which which allowed me to pull together articles across time for examanation. I also transcribed scores of articles which were particulary relevant. I suspect it took not *too much* more time than encoding the material I had transcribed or even all that much more time than using Tesserat to turn my "pictures" of the newspaper into text. (Although Shawn Anctil did urge on me exploring software that would would "automate the boring stuff"). 
This takes me to python (as recommended by Shawn), Tesseract, API, JSO and Regular Expressions. Tesseract did as good a job as I expected on the war diary but the result for the rural Journal was effectively useless. I did not achieve much of API aeither. What little I did in modules 2 and 3 left me with the sense that there is potential here. It is possible this may have saved me a great deal of time. On the other hand, the transcriptions I did do on key articles left me reallly knowing them in a way an automated process would not have. (looks much like material I imported years ago by copying and pasting into Word from a file for which I would use Find/Replace to fix the errors which tended to be regular and then doing a Spellcheck to fix a good portion of the rest - and then read it through to catch the last stuff). If Regular expression can work, it would be faster
Visualizing data is something I had to do for both the MA and the PhD. I tried different approaches based on the capabilities which came packaged in Excel and chose what I thought made sense. I found the discussion in the module useful for intellecualizing the kind of decision-making I had to do but it would not have changed what I did. Again know your audience. I don't see my particular supervisor dealing well with a sonification of my data although I might have been able to sell a word cloud. The drawbacks really lie in the software used. This became clear when I discovered Excel had removed the capability of black-and-white graphing. Pretty colours which do not repoduce well had become the name of the game. What I *would* have done differently had I had the time for the MA was learn some good map-making software. The hours I spent copying via transparency drafting paper a soil map of Prescott County and then adding the villages and towns with the main highways and finally a map with circles to show economic growth of the towns! Marrying geographic and non-geographic information is something I expect to continue to do and it is something I would profit from learning.
The course has been frustrating. I lost so much time in the first two weeks due to technical errors and not "seeing" highlighted material intended to bring me to work tools (for example the textwrangler template). The exercises cannot be hurried through and so I fell further behind. I ended by not exploring deeply enough material that is potentially useful. I also had no time for a capstone exercise where I might have applied these material with the war diaries. Again going back to my days with Shawn Anctil, he had pointed me to the Programming Historian. I started to follow it the summer after the comps when I had more time and the dissertation seemed far away. It is time to go back to it and teach myself more.
