#What I have Achieved and What I have Learned#
I simply have lost too much time and fallen too far behind to attempt a cap exercise. What I can do to conclude is try to sum up what I have learned. I also have been thinking about what would have changed when I worked on my MA and PhD, had I beeen immersed in digital history when I started. Perhaps we could say what the grad student of ten years from now might be doing as a matter of course.
I have been introduced to new places in which to work -- for example Git Hub where I am now. So I have gone beyond dropbox. I am not convinced though that I will use these. I am either old-fashioned or paranoid. Hard to say which but while I do not trust the big corporations, I also do not trust the cloud. My data still sits on a physical server somewhere. Years ago I worked as an Industry Canada representative on a project dealing with lawful intercept of telecom and internet communications. I came out of that project even more distrustful than I had been going in. I want my "stuff" to be under my physical control.
I was forced in the course to articulate my reaction against large claims that digital history is wonderfully revolutionary. I found as I read more deeply that those writing on the subject were not naively wearing rose-glasses. Perhaps the large claims have been necessary to bring attention to the potential and the credibiity of digital history. My own sense as I worked through the assigned readings is that digital history is an extension not a fundamental change. The  requirement that evidence be the central building block of arguments, that we question the evidence and that we ask the so-what questions are as important as ever. There is one thing that has changed and that is the range of forms in which sources come to us.  Again, however, it is an extension. Just as in the past historians had to shift from reading the physical document itself, with all the attendent dust and grime, to squinting at microfiches and microforms, so historians now are stretched further to understand digitized records. We still have the same questions - why was the source created in the first place, why was it preserved across the years and, now, why do we have access to it in whatever format, whether original, microform/microfiche or digital. And for all of these points the same questions have to be answered -- by whom, for whom, in what context, for what purpose, what has been lost versus what has been preserved.
Working in markdown, dillinger and GitHub has brought me face to face with my unthinking acceptance of Word. I realize I have enjoyed too much the sense of accomplishment by seeing my work immediately "look good" and have not considered the price of this polish. Certainly I grumble about the defaults which force me into Word's idea of the proper font, spacing between paragraphs and so on. As a TA I exhorted students to turn off defaults and was amazed each year at the ignorance of many in this computer-savy generation with the workings of the software they use. As I did lay-out of an Association's newsletters for which I was the volunteer editor, I cursed it for its unfitness for the task. And I have suffered near melt-downs when time after time it threw my dissertation into an endless pagination due to a particular graphic in chapter five. Yet there are some advantages in GUI-created documents. Automatic save is a wonderful device and works far more reliably than me remembering to stop and save. Undo is another marvelous function which I often wish worked in real life but which I have discovered does not work in GitHub when I accidently erased an entire line. I may continue to use Text Wrangler which I installed earlier at the suggestion of Shawn Anctil (although it appears it is already "retired"). I am certain I would not have used GitHub to exchange drafts and comments with my dissertation supervisor. As Sean Graham has said elsewhere in his final comments -- know your audience.
Where I really found grist for my mill was in modules two and three -- the finding and cleaning up of data. I was simply amazed to see an entire section of the war diaries appear on my computer while I used the five minutes it took to make a quick lunch. It is also scary. I have authored a virtual exhibit - mobilityhistories on omeka. Had I mirrored it using wget I would have saved myself time this winter when I had to rebuild the exhibit. I had kept a Word version of the text with notes as to the file names of the graphics so it was a case of working back and forth. Using a mirrored version would have been faster. At the same time, anyone can do to me what I did to LAC. I have pride of authorship so would not like to see my work stolen or misrepresented. As well, though, I have graphics in this exhibit which are not mine. I was given permission to use them by various entities. How much responsibility do I have to protect this material from misuse?
Text Encoding was neat and fun, although I found the specific commands difficult to read both on the screen and in the printed version (a running problem for me has been distinguishing between i, l, and the numeral 1 due to dyslexia made worse by poor eyesight). The exercise called for us to simply copy the commands (I could not copy/paste as Safari was not allowing that back-and-forth between the workbook and the dhbox) and then replace with the appropriate material for the page being dissected. This would not do for carrying this kind of work forward as I would want finer breakdowns of categories. For example from my dissertation, I built an index of news items by topic as I read my newspaper. This included differentiating between events on which it was reporting and events it initiated. There was also a clear and visible difference between its editorial opinion writing and its muckraking advocacy. These additional categories probably can be encoded but I would have to dissect what this command is made up of and how it works. The module does provide a link to a "short and sweet handout" which I still need to look at. I also had to be aware of time when I was doing my close reading of the newspaper. Transcribing relevant material from ten years of newspaper articles would have been daunting.

#From this past week's fail log - on attempting to ocr the Ottawa Valley Journal#
Have successfully used wget. Trying the command again to get a copy of Canadian Printer and Publisher. [The site is set up for downloading an entire issue in whatever format one wants. So wget not necessary if I wanted just a few issues - my situation for the dissertation). But it's a test case. The lack of potential targets (ie., digitized documents of interest for my disseration) indicates to me I wouild have had a hard time using wget for doing my dissertation research - the sources simply were not out there on the net.] Made directory in DH called "PrinterPublisher" and cd into it. Decided to pull the first 20 of the 570 pages available. [Error - Not finished and then accidently kicked myself out of dhbox so lost the history. Google search on the error indicated the % dominates in the command so it appears the system read so far and then could not go to the next step - a syntax error]

Exercise 6 and Tesseract - Duplicating two issues of the newspaper from my computer, one jpg and one pdf, deleting all but two pages with rural content and exporting the pdf as jpeg. I then dragged the the files onto dhbox home/deejsmith/
Set up Tesseract and ImageMagick - lines 1 to 4 to import tesseract and image magick from the workbook. [had to redo the image magic import as it was not working. had forgotten to use the sudo command.]
Use image magick on the newspaper files with conversion command [went nowhere - first attempt had misspelled the command and second misspelled the file name]. 
Doing the command as laid out in the workbook with the war diary file. [This time no hiccups but no action. The instructions said be patient. I was patient but the computer timed out instead.]
*Beginning again on Saturday.*
Reimport Tesseract and ImageMagick. Reinput the conversion command correcting a possible misreading. [It worked and did about as reasonable a job as I expected on the war diary - would not take much to clean this up.] Repeated the exercise substituting my file names to convert the newspaper. [Worked but the result is unreadable].

Exercise 4 APIs and JSON - Open binder [problem as the system had problem opening. Took several tries to get it open]. Opened Notebook but what is it doing? Not clear so read getting started and finally understand this is an nest of commands which you step through by clicking run for each command rather than launch to run the entire package. Clicked through and got the results. Substituted "Good Roads" and clicked through. Followed directions and converted result into a csv file. [Did not get as much as I expected.]
Clean up the csv to bring the results into rows under each other so can sort on date. [am surprised at the limited number of years covered, only 1910 to 1921. Raises question as whether I failed to get all the material on the site on the Good Roads campaigns or is it a limitation of the fonds ie only a few newspapers with incomplete years - a pointer that just because you can scrape data doesn't mean you don't need to take time to know what a collection holds and what it doesn't].

Exercise 6 and RStudio - Installing the various packages. Libcur not found (had confused 14 for l4). Doing command again and Installing remainder following the commands as laid out in the workbook. All fine.
Enter RStudio. Unlike the video I am being prompted for a user name and password. I put in the dh name/password (same as the command line log-in) but it was not recognized. Check Slackbot. No-one in the class identified a similar problem. Googled Error incorrect password. People did report a problem to RStudio support. Various different commands suggested 1) verify the installation  - tried as given $ sudo rstudio-server verify-installation [ERROR system error 98 (Address already in use); OCCURRED AT: core::Error core::http::initTcpIpAcceptor(core::http::SocketAcceptorService<boost::asio::ip::tcp>&, const std::string&, const std::string&) /home/ubuntu/rstudio/src/cpp/core/include/core/http/TcpIpSocketUtils.hpp:102; LOGGED FROM: int main(int, char* const*) /home/ubuntu/rstudio/src/cpp/server/ServerMain.cpp:423] 2) try to mirror the command line passwork - $ sudo cp /etc/pam.d/login /etc/pam.d/rstudio [No error message, return to the $ line so tried to log in again. No joy.] Tried in different browser. No difference. At this point too little time left so I left it.

Module 3 Exercise 1 RegEx - Copy the command. [Got a file texas.txt as expected. On opening it, no content]
Delete the empty file and re-enter commands. Worked – had misread the command the first time
Open in Nano, select text and delete [Worked]. As per video, shift to using text editor - download, open, select index and save into new file. Upload. [Worked.]
Regex command copy from workbook [error - I had mistaken an i for a 1]
Redo watching to type correctly [error - deejsmith@13782ddd8316:~$ sed -r -i.bak 's/(.+\bto\b.+)/~\1/g' texas.txtsed: can't read texas.txt: No such file or directory - I had forgotten the file was renamed texas-2.txt
Redo command [nil response and I have run out of time]. Read through the various commands and see potential but cannot practice it.

#More Thoughts on The Exercises#
Cleaning up messy data was something I did for the dissertation by eye, reading for my sins every page of nearly forty years of the rural newspaper which had been preserved on low quality microfilm. I built an index as I read which which allowed me to pull together articles across time for examanation. I also transcribed scores of articles which were particulary relevant. I suspect it took not *too much* more time than encoding the material as per DEI or all that much more time than using Tesserat to turn my "pictures" of the newspaper into text, given the poor quality results. (Although Shawn Anctil did urge on me exploring software that would would "automate the boring stuff"). 
This takes me to python (as recommended by Shawn), Tesseract, API, JSO and Regular Expressions. Tesseract did as good a job on the war diary but the result for the rural Journal was effectively useless (The War Diary looks much like material I imported years ago by copying and pasting from a digitized file into Word and then I used Dind/Replace to fix the "expected errors (eg changing "ing" into "mg"m then doing a Spellcheck to fix a good portion of the rest - and then read it through to catch the last stuff). If I can get Regex to work, it would be faster. Yet the little I did in modules 2 and 3 left me with the sense that there is potential here. It is possible "automating the boring stuff" would have saved me a great deal of time. On the other hand, the transcriptions I did do on key articles left me reallly knowing them in a way an automated process would not have. 
Visualizing data is something I had to do for both the MA and the PhD. I tried different approaches based on the capabilities which came packaged in Excel and chose what I thought made sense. I found the discussion in the module useful for intellecualizing the kind of decision-making I had to do but it would not have changed what I did. Again know your audience. I don't see my supervisor dealing well with a sonification of my data although I might have been able to sell a word cloud. The drawbacks really lie in the software used. This became clear when I discovered Excel had removed the capability of black-and-white graphing. Pretty colours which do not repoduce well had become the name of the game. What I *would* have done differently had I had the time for the MA was learn some good map-making software. The hours I spent copying via transparency drafting paper a soil map of Prescott County and then adding the villages and towns with the main highways and finally a map with circles to show economic growth of the towns!! Marrying geographic and non-geographic information is something I expect to continue to do and it is something I would profit from learning.
The course has been frustrating. I lost so much time in the first two weeks due to technical errors and not "seeing" highlighted material intended to bring me to work tools (for example the textwrangler template). The exercises cannot be hurried through and so I fell further behind. I ended by not exploring deeply enough material that is potentially useful. I also had no time for a capstone exercise where I might have applied these exercises with the war diaries. Again going back to my days with Shawn Anctil, he had pointed me to the Programming Historian. I started to follow it the summer after the comps when I had more time and the dissertation seemed far away. It is time to go back to it and teach myself more
